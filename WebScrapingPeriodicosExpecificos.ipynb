{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMGrLEXiyOKtznYMi8ch1/Z",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RicardoHernandezRodriguez/Hackated/blob/main/WebScrapingPeriodicosExpecificos.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xlQCsboGdaSh",
        "outputId": "b46a8be5-8f6d-4244-d92b-59de911e11dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== B√öSQUEDA EN PERI√ìDICOS ESPEC√çFICOS ===\n",
            "\n",
            "üöÄ Iniciando b√∫squeda en peri√≥dicos espec√≠ficos...\n",
            "\n",
            "üîç Buscando en El Universal...\n",
            "‚úÖ El Universal: 0 noticias encontradas\n",
            "‚úÖ Completado: scrape_el_universal\n",
            "üîç Buscando en El Economista...\n",
            "‚úÖ El Economista: 0 noticias encontradas\n",
            "‚úÖ Completado: scrape_el_economista\n",
            "üîç Buscando en Milenio...\n",
            "‚úÖ Milenio: 0 noticias encontradas\n",
            "‚úÖ Completado: scrape_milenio_economia\n",
            "\n",
            "üìä Total de noticias √∫nicas: 0\n",
            "üéØ B√∫squeda espec√≠fica con t√©rminos: Trump, aranceles, comercio M√©xico, peso mexicano, USMCA\n",
            "üöÄ Iniciando b√∫squeda en peri√≥dicos espec√≠ficos...\n",
            "\n",
            "üîç Buscando en El Universal...\n",
            "‚úÖ El Universal: 0 noticias encontradas\n",
            "‚úÖ Completado: scrape_el_universal\n",
            "üîç Buscando en El Economista...\n",
            "‚úÖ El Economista: 0 noticias encontradas\n",
            "‚úÖ Completado: scrape_el_economista\n",
            "üîç Buscando en Milenio...\n",
            "‚úÖ Milenio: 0 noticias encontradas\n",
            "‚úÖ Completado: scrape_milenio_economia\n",
            "\n",
            "üìä Total de noticias √∫nicas: 0\n",
            "üîç Noticias que contienen los t√©rminos: 0\n",
            "\n",
            "üèÜ TOP 10 NOTICIAS M√ÅS RELEVANTES:\n",
            "------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import time\n",
        "from datetime import datetime\n",
        "from typing import List, Dict\n",
        "import re\n",
        "from urllib.parse import urljoin, urlparse\n",
        "\n",
        "class ScrapingPeriodicos:\n",
        "    def __init__(self):\n",
        "        self.headers = {\n",
        "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
        "            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n",
        "            'Accept-Language': 'es-MX,es;q=0.8,en;q=0.5,en-US;q=0.3',\n",
        "            'Accept-Encoding': 'gzip, deflate',\n",
        "            'Connection': 'keep-alive',\n",
        "        }\n",
        "        self.session = requests.Session()\n",
        "        self.session.headers.update(self.headers)\n",
        "\n",
        "    def scrape_el_universal(self) -> List[Dict]:\n",
        "        \"\"\"Scraping espec√≠fico para El Universal\"\"\"\n",
        "        print(\"üîç Buscando en El Universal...\")\n",
        "        noticias = []\n",
        "\n",
        "        try:\n",
        "            # M√∫ltiples secciones relevantes\n",
        "            urls_secciones = [\n",
        "                'https://www.eluniversal.com.mx/cartera',  # Econom√≠a\n",
        "                'https://www.eluniversal.com.mx/mundo',    # Internacional\n",
        "                'https://www.eluniversal.com.mx/nacion'    # Nacional\n",
        "            ]\n",
        "\n",
        "            for url_seccion in urls_secciones:\n",
        "                try:\n",
        "                    response = self.session.get(url_seccion, timeout=10)\n",
        "                    response.raise_for_status()\n",
        "                    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "                    # El Universal usa diferentes estructuras, probamos varias\n",
        "                    selectores = [\n",
        "                        'article.story-item',\n",
        "                        'div.story-item',\n",
        "                        'article[class*=\"story\"]',\n",
        "                        'div[class*=\"nota\"]',\n",
        "                        'div.field-items div.field-item'\n",
        "                    ]\n",
        "\n",
        "                    articulos_encontrados = []\n",
        "                    for selector in selectores:\n",
        "                        articulos = soup.select(selector)\n",
        "                        if articulos:\n",
        "                            articulos_encontrados = articulos[:15]  # Limitar a 15 por secci√≥n\n",
        "                            break\n",
        "\n",
        "                    for articulo in articulos_encontrados:\n",
        "                        titulo_elem = (articulo.find('h2') or\n",
        "                                     articulo.find('h3') or\n",
        "                                     articulo.find('h1') or\n",
        "                                     articulo.find('a', class_='story-link'))\n",
        "\n",
        "                        if titulo_elem:\n",
        "                            titulo = titulo_elem.get_text().strip()\n",
        "\n",
        "                            # Buscar el enlace\n",
        "                            link_elem = titulo_elem.find('a') if titulo_elem.name != 'a' else titulo_elem\n",
        "                            if not link_elem:\n",
        "                                link_elem = articulo.find('a')\n",
        "\n",
        "                            if link_elem and link_elem.get('href'):\n",
        "                                url_completa = urljoin('https://www.eluniversal.com.mx', link_elem['href'])\n",
        "\n",
        "                                # Buscar descripci√≥n\n",
        "                                desc_elem = (articulo.find('p') or\n",
        "                                           articulo.find('div', class_='summary') or\n",
        "                                           articulo.find('div', class_='description'))\n",
        "                                descripcion = desc_elem.get_text().strip()[:200] if desc_elem else ''\n",
        "\n",
        "                                # Verificar relevancia\n",
        "                                if self._es_relevante_para_tema(titulo, descripcion):\n",
        "                                    noticia = {\n",
        "                                        'titulo': titulo,\n",
        "                                        'descripcion': descripcion,\n",
        "                                        'url': url_completa,\n",
        "                                        'fuente': 'El Universal',\n",
        "                                        'fecha_publicacion': datetime.now().isoformat(),\n",
        "                                        'seccion': url_seccion.split('/')[-1]\n",
        "                                    }\n",
        "                                    noticias.append(noticia)\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"‚ùå Error en secci√≥n {url_seccion}: {e}\")\n",
        "                    continue\n",
        "\n",
        "                time.sleep(1)  # Pausa entre secciones\n",
        "\n",
        "            print(f\"‚úÖ El Universal: {len(noticias)} noticias encontradas\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error general en El Universal: {e}\")\n",
        "\n",
        "        return noticias\n",
        "\n",
        "    def scrape_el_economista(self) -> List[Dict]:\n",
        "        \"\"\"Scraping espec√≠fico para El Economista\"\"\"\n",
        "        print(\"üîç Buscando en El Economista...\")\n",
        "        noticias = []\n",
        "\n",
        "        try:\n",
        "            # Secciones relevantes de El Economista\n",
        "            urls_secciones = [\n",
        "                'https://www.eleconomista.com.mx/mercados',\n",
        "                'https://www.eleconomista.com.mx/economia',\n",
        "                'https://www.eleconomista.com.mx/internacionales',\n",
        "                'https://www.eleconomista.com.mx/politica'\n",
        "            ]\n",
        "\n",
        "            for url_seccion in urls_secciones:\n",
        "                try:\n",
        "                    response = self.session.get(url_seccion, timeout=10)\n",
        "                    response.raise_for_status()\n",
        "                    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "                    # Selectores espec√≠ficos para El Economista\n",
        "                    selectores = [\n",
        "                        'div.story-item',\n",
        "                        'article.story',\n",
        "                        'div.nota-item',\n",
        "                        'div[class*=\"headline\"]',\n",
        "                        'li.story-item'\n",
        "                    ]\n",
        "\n",
        "                    articulos_encontrados = []\n",
        "                    for selector in selectores:\n",
        "                        articulos = soup.select(selector)\n",
        "                        if articulos:\n",
        "                            articulos_encontrados = articulos[:20]\n",
        "                            break\n",
        "\n",
        "                    # Si no encuentra con selectores, busca enlaces de noticias\n",
        "                    if not articulos_encontrados:\n",
        "                        enlaces_noticias = soup.find_all('a', href=re.compile(r'/[0-9]{4}/[0-9]{2}/[0-9]{2}/'))\n",
        "                        articulos_encontrados = enlaces_noticias[:15]\n",
        "\n",
        "                    for articulo in articulos_encontrados:\n",
        "                        if articulo.name == 'a':\n",
        "                            # Si el elemento ya es un enlace\n",
        "                            titulo = articulo.get_text().strip()\n",
        "                            url_completa = urljoin('https://www.eleconomista.com.mx', articulo['href'])\n",
        "                            descripcion = ''\n",
        "                        else:\n",
        "                            # Buscar t√≠tulo dentro del art√≠culo\n",
        "                            titulo_elem = (articulo.find('h2') or\n",
        "                                         articulo.find('h3') or\n",
        "                                         articulo.find('h1') or\n",
        "                                         articulo.find('a'))\n",
        "\n",
        "                            if not titulo_elem:\n",
        "                                continue\n",
        "\n",
        "                            titulo = titulo_elem.get_text().strip()\n",
        "\n",
        "                            # Buscar enlace\n",
        "                            link_elem = titulo_elem if titulo_elem.name == 'a' else articulo.find('a')\n",
        "                            if not link_elem or not link_elem.get('href'):\n",
        "                                continue\n",
        "\n",
        "                            url_completa = urljoin('https://www.eleconomista.com.mx', link_elem['href'])\n",
        "\n",
        "                            # Buscar descripci√≥n\n",
        "                            desc_elem = articulo.find('p', class_='summary') or articulo.find('p')\n",
        "                            descripcion = desc_elem.get_text().strip()[:200] if desc_elem else ''\n",
        "\n",
        "                        # Verificar relevancia y evitar duplicados\n",
        "                        if (titulo and len(titulo) > 10 and\n",
        "                            self._es_relevante_para_tema(titulo, descripcion) and\n",
        "                            not any(n['url'] == url_completa for n in noticias)):\n",
        "\n",
        "                            noticia = {\n",
        "                                'titulo': titulo,\n",
        "                                'descripcion': descripcion,\n",
        "                                'url': url_completa,\n",
        "                                'fuente': 'El Economista',\n",
        "                                'fecha_publicacion': datetime.now().isoformat(),\n",
        "                                'seccion': url_seccion.split('/')[-1]\n",
        "                            }\n",
        "                            noticias.append(noticia)\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"‚ùå Error en secci√≥n {url_seccion}: {e}\")\n",
        "                    continue\n",
        "\n",
        "                time.sleep(1)  # Pausa entre secciones\n",
        "\n",
        "            print(f\"‚úÖ El Economista: {len(noticias)} noticias encontradas\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error general en El Economista: {e}\")\n",
        "\n",
        "        return noticias\n",
        "\n",
        "    def scrape_milenio_economia(self) -> List[Dict]:\n",
        "        \"\"\"Scraping espec√≠fico para la secci√≥n de econom√≠a de Milenio\"\"\"\n",
        "        print(\"üîç Buscando en Milenio...\")\n",
        "        noticias = []\n",
        "\n",
        "        try:\n",
        "            url = 'https://www.milenio.com/negocios'\n",
        "            response = self.session.get(url, timeout=10)\n",
        "            response.raise_for_status()\n",
        "            soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "            # Milenio tiene una estructura m√°s consistente\n",
        "            articulos = soup.find_all(['article', 'div'], class_=re.compile(r'(story|nota|news)'))[:15]\n",
        "\n",
        "            for articulo in articulos:\n",
        "                titulo_elem = (articulo.find('h2') or\n",
        "                             articulo.find('h3') or\n",
        "                             articulo.find('h1'))\n",
        "\n",
        "                if titulo_elem:\n",
        "                    # Obtener texto del t√≠tulo\n",
        "                    link_elem = titulo_elem.find('a')\n",
        "                    if link_elem:\n",
        "                        titulo = link_elem.get_text().strip()\n",
        "                        url_completa = urljoin('https://www.milenio.com', link_elem['href'])\n",
        "                    else:\n",
        "                        titulo = titulo_elem.get_text().strip()\n",
        "                        link_elem = articulo.find('a')\n",
        "                        if not link_elem:\n",
        "                            continue\n",
        "                        url_completa = urljoin('https://www.milenio.com', link_elem['href'])\n",
        "\n",
        "                    # Buscar descripci√≥n\n",
        "                    desc_elem = articulo.find('p')\n",
        "                    descripcion = desc_elem.get_text().strip()[:200] if desc_elem else ''\n",
        "\n",
        "                    if self._es_relevante_para_tema(titulo, descripcion):\n",
        "                        noticia = {\n",
        "                            'titulo': titulo,\n",
        "                            'descripcion': descripcion,\n",
        "                            'url': url_completa,\n",
        "                            'fuente': 'Milenio',\n",
        "                            'fecha_publicacion': datetime.now().isoformat(),\n",
        "                            'seccion': 'negocios'\n",
        "                        }\n",
        "                        noticias.append(noticia)\n",
        "\n",
        "            print(f\"‚úÖ Milenio: {len(noticias)} noticias encontradas\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error en Milenio: {e}\")\n",
        "\n",
        "        return noticias\n",
        "\n",
        "    def extraer_contenido_completo(self, url: str, fuente: str) -> str:\n",
        "        \"\"\"Extrae el contenido completo de un art√≠culo espec√≠fico\"\"\"\n",
        "        try:\n",
        "            response = self.session.get(url, timeout=15)\n",
        "            response.raise_for_status()\n",
        "            soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "            # Selectores comunes para el contenido del art√≠culo\n",
        "            selectores_contenido = [\n",
        "                'div.story-body',\n",
        "                'div.article-body',\n",
        "                'div.content',\n",
        "                'div.nota-content',\n",
        "                'div[class*=\"body\"]',\n",
        "                'section.article-content',\n",
        "                'div.entry-content'\n",
        "            ]\n",
        "\n",
        "            contenido = \"\"\n",
        "            for selector in selectores_contenido:\n",
        "                elemento = soup.select_one(selector)\n",
        "                if elemento:\n",
        "                    # Limpiar scripts y elementos no deseados\n",
        "                    for script in elemento([\"script\", \"style\", \"aside\", \"nav\"]):\n",
        "                        script.decompose()\n",
        "\n",
        "                    paragrafos = elemento.find_all('p')\n",
        "                    contenido = ' '.join([p.get_text().strip() for p in paragrafos if p.get_text().strip()])\n",
        "                    break\n",
        "\n",
        "            # Si no encuentra contenido estructurado, busca todos los p√°rrafos\n",
        "            if not contenido:\n",
        "                paragrafos = soup.find_all('p')\n",
        "                contenido = ' '.join([p.get_text().strip() for p in paragrafos if len(p.get_text().strip()) > 50])\n",
        "\n",
        "            return contenido[:1000] if contenido else \"No se pudo extraer contenido\"\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error extrayendo contenido de {url}: {e}\")\n",
        "            return \"Error al extraer contenido\"\n",
        "\n",
        "    def buscar_en_todos_los_periodicos(self) -> List[Dict]:\n",
        "        \"\"\"Ejecuta scraping en todos los peri√≥dicos configurados\"\"\"\n",
        "        print(\"üöÄ Iniciando b√∫squeda en peri√≥dicos espec√≠ficos...\\n\")\n",
        "\n",
        "        todas_noticias = []\n",
        "\n",
        "        # Ejecutar scraping en cada peri√≥dico\n",
        "        periodicos = [\n",
        "            self.scrape_el_universal,\n",
        "            self.scrape_el_economista,\n",
        "            self.scrape_milenio_economia\n",
        "        ]\n",
        "\n",
        "        for scraper in periodicos:\n",
        "            try:\n",
        "                noticias = scraper()\n",
        "                todas_noticias.extend(noticias)\n",
        "                print(f\"‚úÖ Completado: {scraper.__name__}\")\n",
        "                time.sleep(2)  # Pausa entre peri√≥dicos\n",
        "            except Exception as e:\n",
        "                print(f\"‚ùå Error en {scraper.__name__}: {e}\")\n",
        "\n",
        "        # Eliminar duplicados por URL\n",
        "        noticias_unicas = []\n",
        "        urls_vistas = set()\n",
        "\n",
        "        for noticia in todas_noticias:\n",
        "            if noticia['url'] not in urls_vistas:\n",
        "                noticias_unicas.append(noticia)\n",
        "                urls_vistas.add(noticia['url'])\n",
        "\n",
        "        print(f\"\\nüìä Total de noticias √∫nicas: {len(noticias_unicas)}\")\n",
        "        return noticias_unicas\n",
        "\n",
        "    def _es_relevante_para_tema(self, titulo: str, descripcion: str) -> bool:\n",
        "        \"\"\"Verifica si la noticia es relevante para el tema de inter√©s\"\"\"\n",
        "        texto_completo = (titulo + ' ' + descripcion).lower()\n",
        "\n",
        "        # Palabras clave relacionadas con Trump/EE.UU. y econom√≠a\n",
        "        keywords_politica = ['trump', 'estados unidos', 'eeuu', 'washington', 'biden', 'presidente']\n",
        "        keywords_economia = ['mercado', 'econom√≠a', 'comercio', 'aranceles', 'd√≥lar', 'peso', 'bolsa', 'inflaci√≥n']\n",
        "        keywords_mexico = ['m√©xico', 'mexico', 'mexicano', 'nacional']\n",
        "\n",
        "        # Debe tener al menos una palabra de cada categor√≠a O ser sobre M√©xico + econom√≠a\n",
        "        tiene_politica = any(word in texto_completo for word in keywords_politica)\n",
        "        tiene_economia = any(word in texto_completo for word in keywords_economia)\n",
        "        tiene_mexico = any(word in texto_completo for word in keywords_mexico)\n",
        "\n",
        "        return (tiene_politica and tiene_economia) or (tiene_mexico and tiene_economia)\n",
        "\n",
        "    def buscar_con_terminos_especificos(self, terminos: List[str]) -> List[Dict]:\n",
        "        \"\"\"Busca noticias que contengan t√©rminos espec√≠ficos\"\"\"\n",
        "        print(f\"üéØ B√∫squeda espec√≠fica con t√©rminos: {', '.join(terminos)}\")\n",
        "\n",
        "        todas_noticias = self.buscar_en_todos_los_periodicos()\n",
        "        noticias_filtradas = []\n",
        "\n",
        "        for noticia in todas_noticias:\n",
        "            texto_completo = (noticia['titulo'] + ' ' + noticia['descripcion']).lower()\n",
        "\n",
        "            # Verificar si contiene alguno de los t√©rminos espec√≠ficos\n",
        "            if any(termino.lower() in texto_completo for termino in terminos):\n",
        "                noticias_filtradas.append(noticia)\n",
        "\n",
        "        print(f\"üîç Noticias que contienen los t√©rminos: {len(noticias_filtradas)}\")\n",
        "        return noticias_filtradas\n",
        "\n",
        "# Ejemplo de uso espec√≠fico\n",
        "if __name__ == \"__main__\":\n",
        "    scraper = ScrapingPeriodicos()\n",
        "\n",
        "    print(\"=== B√öSQUEDA EN PERI√ìDICOS ESPEC√çFICOS ===\\n\")\n",
        "\n",
        "    # Opci√≥n 1: Buscar en todos los peri√≥dicos\n",
        "    noticias_generales = scraper.buscar_en_todos_los_periodicos()\n",
        "\n",
        "    # Opci√≥n 2: Buscar t√©rminos espec√≠ficos\n",
        "    terminos_especificos = ['Trump', 'aranceles', 'comercio M√©xico', 'peso mexicano', 'USMCA']\n",
        "    noticias_especificas = scraper.buscar_con_terminos_especificos(terminos_especificos)\n",
        "\n",
        "    # Mostrar resultados\n",
        "    print(\"\\nüèÜ TOP 10 NOTICIAS M√ÅS RELEVANTES:\")\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "    for i, noticia in enumerate(noticias_especificas[:10], 1):\n",
        "        print(f\"{i}. [{noticia['fuente']}] {noticia['titulo']}\")\n",
        "        print(f\"   üìÑ {noticia['descripcion'][:100]}...\")\n",
        "        print(f\"   üîó {noticia['url']}\")\n",
        "        print(f\"   üìÖ Secci√≥n: {noticia.get('seccion', 'N/A')}\\n\")\n",
        "\n",
        "    # Extraer contenido completo de una noticia espec√≠fica\n",
        "    if noticias_especificas:\n",
        "        print(\"üìñ EXTRAYENDO CONTENIDO COMPLETO DE LA PRIMERA NOTICIA:\")\n",
        "        print(\"-\" * 60)\n",
        "        primera_noticia = noticias_especificas[0]\n",
        "        contenido_completo = scraper.extraer_contenido_completo(\n",
        "            primera_noticia['url'],\n",
        "            primera_noticia['fuente']\n",
        "        )\n",
        "        print(f\"Contenido: {contenido_completo[:500]}...\")"
      ]
    }
  ]
}