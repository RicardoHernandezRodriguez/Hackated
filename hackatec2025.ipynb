{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOKNxy7cYfyaPPo8yPWYE5x",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RicardoHernandezRodriguez/Hackated/blob/main/hackatec2025.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qbs4ljFzPo6X",
        "outputId": "b14a8ef0-7358-45fd-af52-93f964dfa9d3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîç Iniciando b√∫squeda en m√∫ltiples fuentes...\n",
            "\n",
            "‚ö†Ô∏è  NewsAPI key no proporcionada. Usando fuentes gratuitas.\n",
            "‚úÖ Google News: 8 noticias encontradas\n",
            "‚úÖ Google News: 3 noticias encontradas\n",
            "‚úÖ Google News: 8 noticias encontradas\n",
            "‚úÖ Google News: 1 noticias encontradas\n",
            "‚úÖ El Universal: 0 noticias encontradas\n",
            "\n",
            "üìä Total de noticias √∫nicas encontradas: 19\n",
            "üíæ 19 noticias nuevas guardadas en la base de datos\n",
            "\n",
            "üìà RESUMEN POR CATEGOR√çAS:\n",
            "--------------------------------------------------\n",
            "Comercio Internacional: 8 noticias (Relevancia promedio: 4.2)\n",
            "Tipo de Cambio: 4 noticias (Relevancia promedio: 3.0)\n",
            "Econom√≠a General: 4 noticias (Relevancia promedio: 3.8)\n",
            "Mercados Financieros: 2 noticias (Relevancia promedio: 5.0)\n",
            "Pol√≠tica Econ√≥mica: 1 noticias (Relevancia promedio: 2.0)\n",
            "üìÑ Reporte generado: reporte_noticias_20250526_153933.xlsx\n",
            "\n",
            "üî• TOP 5 NOTICIAS M√ÅS RELEVANTES:\n",
            "--------------------------------------------------\n",
            "1. [Comercio Internacional] Econom√≠a prev√© un impacto limitado para M√©xico si Trump pone aranceles a remesas - Milenio\n",
            "   Fuente: Google News | Relevancia: 6/10\n",
            "   URL: https://news.google.com/rss/articles/CBMimAFBVV95cUxQN05SRk9zaF96c25IMzBVUDF3RTg2akFyeW1wYXRhSTlSSnh3cFZPV3BhZXFicWZkVjdYdEwydm5JWDVOd2NkWGtycFVpRXFvMkpKbVBjSGhFZ2szRmdZdG94akh4d2VxREp4TkZ6dG1xaktuZW5pS182Sjc4M0ViUzZRbTFWTzUwOV94OV9LWjVKNFNhSm13OdIBmAFBVV95cUxPb3MwNlAtUXJqMzAwNVBsckl5QjNiRXdXa2MxcnBQRldfd1UweUlPLVM0T0V0LXlJdnRyYm1sbDZONWFVY0xHYXNxbXloTVBUVEhpeHVkdU9Vczdkc3BkVWU4UXNJWXhvTnRZZGNLUTJLX0xfSGZTVXBhWjJ0aXdkNFhjOXVPaHNNWGFfTE5PM2RXdjItdVRjSQ?oc=5\n",
            "\n",
            "2. [Comercio Internacional] Econom√≠a - ¬øM√©xico, hacia una recesi√≥n?: el impacto de los aranceles de Trump en el PIB mexicano - France 24\n",
            "   Fuente: Google News | Relevancia: 6/10\n",
            "   URL: https://news.google.com/rss/articles/CBMi3gFBVV95cUxPMUdDMkN0VXpxYUxCOWhjZTMzbl93Q2kyQjFCNHZtZHQwTzhsRXVpQU5KU3pDVXhJRnVWcG9nNkpEM2RzWXlWc3FJVnp1elV5R29jVnFEYTM3T3gtck9uSXVLczNxT3F4VTVXRDZlMXRSVmNRYjV3VkIydjB2YmkyQmtxXzhkUG9wd3otUFJsc3JVMUZGWnVSZ0x6S3F5aWZjM0NSVXBQNy1sakxEVUNUTGxSRVgxMUpORFlSQXBYWWlOcERFR2RsUHhTZkhTVEZiZnpzZEl4NTk0czhGbEE?oc=5\n",
            "\n",
            "3. [Comercio Internacional] Peso mexicano hoy, 23 de mayo: cae frente al d√≥lar por amenaza de aranceles de Trump - expansion.mx\n",
            "   Fuente: Google News | Relevancia: 6/10\n",
            "   URL: https://news.google.com/rss/articles/CBMifkFVX3lxTFBGZzRmWmNhVVFod0VkYnY4VUpwbFpUbVFnNGItVENoSXM5bzRYYmQ2X2E3YjZVU1hjMVdTb0NUc1dmWXlPWTRCb3lWS2ZQVDV1MnlfWFhITjVpcHYzYTRkcXRjQWctSzNnbU5NeGFCdFdSMTA1blJFd0R6S1NqQdIBiwFBVV95cUxPcU1wSkw1MTNkLWphMlFlamY5eFpNZUJwaC1aVXV3SEV6QWdmWVBvVVNDb1IzNC1ScDBLcUlQZVpFMXNlV2U3NHBLZGk4c3Bzbzhvc1BCclE1MlhmRVVlR2ZYQmRCeU1OQmtNbjd3SkMxTFl2YWhOSEEzekh3ZDF0QXNqV2VtbGxTQzZ3?oc=5\n",
            "\n",
            "4. [Mercados Financieros] Peso mexicano retrocede contra el d√≥lar; mercado sigue proyecto fiscal de Trump - Yahoo Noticias\n",
            "   Fuente: Google News | Relevancia: 5/10\n",
            "   URL: https://news.google.com/rss/articles/CBMilAFBVV95cUxOQ1BueEUzMFdlRklQTnZkZWl0aDdJaS05UTNkYlN3WVVtdHowNFNYdjBZdk9RY1YzQTVuRWFDU0dqN3hYbnMtZ3pmTUhIVGlrdHRadlpMTFFUMDVQOF9HVVRuOUNIWnhPVWplR1B4VHlMT05hVHlSOU5yYUsxNDJ5SnBaNnh2OVpMaEtpTnlJSTBCRkRa?oc=5\n",
            "\n",
            "5. [Mercados Financieros] Peso mexicano retrocede contra el d√≥lar; mercado sigue proyecto fiscal de Trump - El Economista\n",
            "   Fuente: Google News | Relevancia: 5/10\n",
            "   URL: https://news.google.com/rss/articles/CBMiwwFBVV95cUxQMTlOUktsWXJUN2RiSHY5R1UxZnJXdlp3TEU5Y0hqNjdZeHVCa1hEcFkxb3R0eldmdkJteW8wYzkwLTRVRFFzSjY0enVJT0dFa2E3bkRwQ1JVZ3RtclJPS3FLMHR6V19QQS1qdWozSjRjOWxINm9LalJlR1Y2ZTRuNjlNdExOc2FncF9XS0pYY0hmbUNoMVYyRERjNTN2SVBmbVpxRUZqZ0JGLU1UZHlzUkl2cTVtdUJfNFZ3S2RlT2dhUlHSAcgBQVVfeXFMT0tEc19XelZRaXUyc0VOSGVmXzJDRngzLWZFU21xQ0ZiajJhVVBfXzViS0ZlWjMxRkF3YUNfODFzaGFTaXRGT1EtWThka3cwdWhzUEs2cTdEdmU2T3IyUEpZUU1hanplbXNpZjZyWUxNQ2VhX05oWHQ0YlVUMGRpS21PeTRGWFN1TklmUmtBZVhia1NmOWFJZ1FYdEtyQkUwS2JHR09STXZFc05VOUQtRDVQWWRPNzdxTTBYRUtzOGlRamJVOUhNb2s?oc=5\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "import json\n",
        "from datetime import datetime, timedelta\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import time\n",
        "from typing import Dict, List, Optional\n",
        "import sqlite3\n",
        "\n",
        "class BuscadorNoticias:\n",
        "    def __init__(self, news_api_key: Optional[str] = None):\n",
        "        self.news_api_key = news_api_key\n",
        "        self.keywords_trump = [\n",
        "            'trump', 'presidente estados unidos', 'donald trump',\n",
        "            'pol√≠tica comercial', 'aranceles', 'comercio m√©xico',\n",
        "            'usmca', 't-mec', 'nafta'\n",
        "        ]\n",
        "        self.keywords_economia = [\n",
        "            'mercado', 'bolsa', 'peso mexicano', 'inflaci√≥n',\n",
        "            'inversi√≥n', 'econom√≠a m√©xico', 'remesas',\n",
        "            'wall street', 'mercados emergentes', 'tipo de cambio'\n",
        "        ]\n",
        "        self.fuentes_mexico = [\n",
        "            'el-universal-mx', 'milenio', 'excelsior',\n",
        "            'el-financiero-mx', 'proceso'\n",
        "        ]\n",
        "        self.fuentes_usa = [\n",
        "            'cnn', 'reuters', 'associated-press', 'bbc-news',\n",
        "            'the-wall-street-journal', 'bloomberg'\n",
        "        ]\n",
        "        self.init_database()\n",
        "\n",
        "    def init_database(self):\n",
        "        \"\"\"Inicializa la base de datos SQLite para almacenar noticias\"\"\"\n",
        "        self.conn = sqlite3.connect('noticias_economia.db')\n",
        "        cursor = self.conn.cursor()\n",
        "        cursor.execute('''\n",
        "            CREATE TABLE IF NOT EXISTS noticias (\n",
        "                id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "                titulo TEXT,\n",
        "                descripcion TEXT,\n",
        "                url TEXT UNIQUE,\n",
        "                fuente TEXT,\n",
        "                fecha_publicacion TEXT,\n",
        "                categoria TEXT,\n",
        "                relevancia INTEGER,\n",
        "                fecha_guardado TEXT\n",
        "            )\n",
        "        ''')\n",
        "        self.conn.commit()\n",
        "\n",
        "    def buscar_con_newsapi(self, dias_atras: int = 7) -> List[Dict]:\n",
        "        \"\"\"Busca noticias usando NewsAPI\"\"\"\n",
        "        if not self.news_api_key:\n",
        "            print(\"‚ö†Ô∏è  NewsAPI key no proporcionada. Usando fuentes gratuitas.\")\n",
        "            return []\n",
        "\n",
        "        url = \"https://newsapi.org/v2/everything\"\n",
        "        fecha_desde = (datetime.now() - timedelta(days=dias_atras)).strftime('%Y-%m-%d')\n",
        "\n",
        "        noticias = []\n",
        "\n",
        "        # B√∫squeda combinada de palabras clave\n",
        "        query = '(trump OR \"presidente estados unidos\") AND (m√©xico OR mexico OR \"mercado\" OR \"econom√≠a\")'\n",
        "\n",
        "        params = {\n",
        "            'q': query,\n",
        "            'from': fecha_desde,\n",
        "            'sortBy': 'relevancy',\n",
        "            'language': 'es,en',\n",
        "            'apiKey': self.news_api_key,\n",
        "            'pageSize': 50\n",
        "        }\n",
        "\n",
        "        try:\n",
        "            response = requests.get(url, params=params)\n",
        "            response.raise_for_status()\n",
        "            data = response.json()\n",
        "\n",
        "            for article in data.get('articles', []):\n",
        "                if self._es_relevante(article['title'], article.get('description', '')):\n",
        "                    noticia = {\n",
        "                        'titulo': article['title'],\n",
        "                        'descripcion': article.get('description', ''),\n",
        "                        'url': article['url'],\n",
        "                        'fuente': article['source']['name'],\n",
        "                        'fecha_publicacion': article['publishedAt'],\n",
        "                        'categoria': self._categorizar_noticia(article['title'], article.get('description', '')),\n",
        "                        'relevancia': self._calcular_relevancia(article['title'], article.get('description', ''))\n",
        "                    }\n",
        "                    noticias.append(noticia)\n",
        "\n",
        "            print(f\"‚úÖ NewsAPI: {len(noticias)} noticias encontradas\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error en NewsAPI: {e}\")\n",
        "\n",
        "        return noticias\n",
        "\n",
        "    def scrape_el_universal(self) -> List[Dict]:\n",
        "        \"\"\"Scraping de El Universal (ejemplo)\"\"\"\n",
        "        noticias = []\n",
        "        try:\n",
        "            url = \"https://www.eluniversal.com.mx/cartera\"\n",
        "            headers = {\n",
        "                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
        "            }\n",
        "\n",
        "            response = requests.get(url, headers=headers)\n",
        "            soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "            # Buscar art√≠culos (esto puede variar seg√∫n la estructura del sitio)\n",
        "            articulos = soup.find_all('article', class_='article-item')[:10]\n",
        "\n",
        "            for articulo in articulos:\n",
        "                titulo_elem = articulo.find('h2') or articulo.find('h3')\n",
        "                if titulo_elem:\n",
        "                    titulo = titulo_elem.get_text().strip()\n",
        "                    if self._es_relevante(titulo, ''):\n",
        "                        link_elem = articulo.find('a')\n",
        "                        url_completa = link_elem['href'] if link_elem else ''\n",
        "\n",
        "                        noticia = {\n",
        "                            'titulo': titulo,\n",
        "                            'descripcion': '',\n",
        "                            'url': url_completa,\n",
        "                            'fuente': 'El Universal',\n",
        "                            'fecha_publicacion': datetime.now().isoformat(),\n",
        "                            'categoria': self._categorizar_noticia(titulo, ''),\n",
        "                            'relevancia': self._calcular_relevancia(titulo, '')\n",
        "                        }\n",
        "                        noticias.append(noticia)\n",
        "\n",
        "            print(f\"‚úÖ El Universal: {len(noticias)} noticias encontradas\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error scraping El Universal: {e}\")\n",
        "\n",
        "        return noticias\n",
        "\n",
        "    def buscar_google_news(self, query: str) -> List[Dict]:\n",
        "        \"\"\"B√∫squeda b√°sica en Google News (sin API)\"\"\"\n",
        "        noticias = []\n",
        "        try:\n",
        "            # Usar RSS de Google News (m√©todo alternativo gratuito)\n",
        "            url = f\"https://news.google.com/rss/search?q={query}&hl=es&gl=MX&ceid=MX:es\"\n",
        "\n",
        "            response = requests.get(url)\n",
        "            soup = BeautifulSoup(response.content, 'xml')\n",
        "\n",
        "            items = soup.find_all('item')[:20]\n",
        "\n",
        "            for item in items:\n",
        "                titulo = item.title.text if item.title else ''\n",
        "                descripcion = item.description.text if item.description else ''\n",
        "\n",
        "                if self._es_relevante(titulo, descripcion):\n",
        "                    noticia = {\n",
        "                        'titulo': titulo,\n",
        "                        'descripcion': descripcion,\n",
        "                        'url': item.link.text if item.link else '',\n",
        "                        'fuente': 'Google News',\n",
        "                        'fecha_publicacion': item.pubDate.text if item.pubDate else '',\n",
        "                        'categoria': self._categorizar_noticia(titulo, descripcion),\n",
        "                        'relevancia': self._calcular_relevancia(titulo, descripcion)\n",
        "                    }\n",
        "                    noticias.append(noticia)\n",
        "\n",
        "            print(f\"‚úÖ Google News: {len(noticias)} noticias encontradas\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error en Google News: {e}\")\n",
        "\n",
        "        return noticias\n",
        "\n",
        "    def _es_relevante(self, titulo: str, descripcion: str) -> bool:\n",
        "        \"\"\"Determina si una noticia es relevante bas√°ndose en palabras clave\"\"\"\n",
        "        texto_completo = (titulo + ' ' + descripcion).lower()\n",
        "\n",
        "        # Debe contener al menos una palabra clave de Trump/Pol√≠tica\n",
        "        tiene_trump = any(keyword in texto_completo for keyword in self.keywords_trump)\n",
        "\n",
        "        # Y al menos una palabra clave econ√≥mica\n",
        "        tiene_economia = any(keyword in texto_completo for keyword in self.keywords_economia)\n",
        "\n",
        "        # O menciones espec√≠ficas de M√©xico en contexto econ√≥mico\n",
        "        tiene_mexico_economia = ('m√©xico' in texto_completo or 'mexico' in texto_completo) and \\\n",
        "                               any(word in texto_completo for word in ['mercado', 'econom√≠a', 'comercio', 'peso'])\n",
        "\n",
        "        return (tiene_trump and tiene_economia) or tiene_mexico_economia\n",
        "\n",
        "    def _categorizar_noticia(self, titulo: str, descripcion: str) -> str:\n",
        "        \"\"\"Categoriza la noticia seg√∫n su contenido\"\"\"\n",
        "        texto = (titulo + ' ' + descripcion).lower()\n",
        "\n",
        "        if any(word in texto for word in ['arancel', 'comercio', 'exportaci√≥n', 'importaci√≥n']):\n",
        "            return 'Comercio Internacional'\n",
        "        elif any(word in texto for word in ['bolsa', 'mercado', 'inversi√≥n', 'wall street']):\n",
        "            return 'Mercados Financieros'\n",
        "        elif any(word in texto for word in ['peso', 'tipo de cambio', 'd√≥lar']):\n",
        "            return 'Tipo de Cambio'\n",
        "        elif any(word in texto for word in ['inflaci√≥n', 'econom√≠a', 'pib']):\n",
        "            return 'Econom√≠a General'\n",
        "        else:\n",
        "            return 'Pol√≠tica Econ√≥mica'\n",
        "\n",
        "    def _calcular_relevancia(self, titulo: str, descripcion: str) -> int:\n",
        "        \"\"\"Calcula relevancia de 1-10 bas√°ndose en palabras clave\"\"\"\n",
        "        texto = (titulo + ' ' + descripcion).lower()\n",
        "        puntuacion = 0\n",
        "\n",
        "        # Palabras clave de alta relevancia\n",
        "        palabras_alta = ['trump', 'aranceles', 'comercio m√©xico', 'peso mexicano', 'wall street']\n",
        "        puntuacion += sum(2 for palabra in palabras_alta if palabra in texto)\n",
        "\n",
        "        # Palabras clave de media relevancia\n",
        "        palabras_media = ['mercado', 'econom√≠a', 'inversi√≥n', 'm√©xico']\n",
        "        puntuacion += sum(1 for palabra in palabras_media if palabra in texto)\n",
        "\n",
        "        return min(puntuacion, 10)\n",
        "\n",
        "    def guardar_noticias(self, noticias: List[Dict]):\n",
        "        \"\"\"Guarda noticias en la base de datos\"\"\"\n",
        "        cursor = self.conn.cursor()\n",
        "        guardadas = 0\n",
        "\n",
        "        for noticia in noticias:\n",
        "            try:\n",
        "                cursor.execute('''\n",
        "                    INSERT OR IGNORE INTO noticias\n",
        "                    (titulo, descripcion, url, fuente, fecha_publicacion, categoria, relevancia, fecha_guardado)\n",
        "                    VALUES (?, ?, ?, ?, ?, ?, ?, ?)\n",
        "                ''', (\n",
        "                    noticia['titulo'],\n",
        "                    noticia['descripcion'],\n",
        "                    noticia['url'],\n",
        "                    noticia['fuente'],\n",
        "                    noticia['fecha_publicacion'],\n",
        "                    noticia['categoria'],\n",
        "                    noticia['relevancia'],\n",
        "                    datetime.now().isoformat()\n",
        "                ))\n",
        "                if cursor.rowcount > 0:\n",
        "                    guardadas += 1\n",
        "            except Exception as e:\n",
        "                print(f\"Error guardando noticia: {e}\")\n",
        "\n",
        "        self.conn.commit()\n",
        "        print(f\"üíæ {guardadas} noticias nuevas guardadas en la base de datos\")\n",
        "\n",
        "    def buscar_todas_las_fuentes(self, dias_atras: int = 7) -> List[Dict]:\n",
        "        \"\"\"Ejecuta b√∫squeda en todas las fuentes disponibles\"\"\"\n",
        "        print(\"üîç Iniciando b√∫squeda en m√∫ltiples fuentes...\\n\")\n",
        "\n",
        "        todas_noticias = []\n",
        "\n",
        "        # NewsAPI (si est√° disponible)\n",
        "        noticias_newsapi = self.buscar_con_newsapi(dias_atras)\n",
        "        todas_noticias.extend(noticias_newsapi)\n",
        "\n",
        "        # Google News\n",
        "        queries = [\n",
        "            'Trump M√©xico econom√≠a',\n",
        "            'aranceles M√©xico comercio',\n",
        "            'mercado peso mexicano',\n",
        "            'presidente Estados Unidos M√©xico'\n",
        "        ]\n",
        "\n",
        "        for query in queries:\n",
        "            noticias_google = self.buscar_google_news(query)\n",
        "            todas_noticias.extend(noticias_google)\n",
        "            time.sleep(1)  # Pausa para evitar bloqueos\n",
        "\n",
        "        # Scraping (ejemplo con El Universal)\n",
        "        noticias_scraping = self.scrape_el_universal()\n",
        "        todas_noticias.extend(noticias_scraping)\n",
        "\n",
        "        # Eliminar duplicados bas√°ndose en URL\n",
        "        noticias_unicas = []\n",
        "        urls_vistas = set()\n",
        "\n",
        "        for noticia in todas_noticias:\n",
        "            if noticia['url'] not in urls_vistas:\n",
        "                noticias_unicas.append(noticia)\n",
        "                urls_vistas.add(noticia['url'])\n",
        "\n",
        "        print(f\"\\nüìä Total de noticias √∫nicas encontradas: {len(noticias_unicas)}\")\n",
        "\n",
        "        return noticias_unicas\n",
        "\n",
        "    def generar_reporte(self, formato: str = 'excel') -> str:\n",
        "        \"\"\"Genera reporte de noticias almacenadas\"\"\"\n",
        "        cursor = self.conn.cursor()\n",
        "        cursor.execute('''\n",
        "            SELECT titulo, descripcion, url, fuente, fecha_publicacion,\n",
        "                   categoria, relevancia, fecha_guardado\n",
        "            FROM noticias\n",
        "            ORDER BY relevancia DESC, fecha_publicacion DESC\n",
        "        ''')\n",
        "\n",
        "        noticias = cursor.fetchall()\n",
        "\n",
        "        if not noticias:\n",
        "            return \"No hay noticias para reportar\"\n",
        "\n",
        "        # Crear DataFrame\n",
        "        df = pd.DataFrame(noticias, columns=[\n",
        "            'T√≠tulo', 'Descripci√≥n', 'URL', 'Fuente', 'Fecha Publicaci√≥n',\n",
        "            'Categor√≠a', 'Relevancia', 'Fecha Guardado'\n",
        "        ])\n",
        "\n",
        "        # Generar archivo\n",
        "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "\n",
        "        if formato == 'excel':\n",
        "            filename = f'reporte_noticias_{timestamp}.xlsx'\n",
        "            df.to_excel(filename, index=False)\n",
        "        else:\n",
        "            filename = f'reporte_noticias_{timestamp}.csv'\n",
        "            df.to_csv(filename, index=False)\n",
        "\n",
        "        print(f\"üìÑ Reporte generado: {filename}\")\n",
        "        return filename\n",
        "\n",
        "    def mostrar_resumen(self):\n",
        "        \"\"\"Muestra resumen de noticias por categor√≠a\"\"\"\n",
        "        cursor = self.conn.cursor()\n",
        "        cursor.execute('''\n",
        "            SELECT categoria, COUNT(*) as cantidad, AVG(relevancia) as relevancia_promedio\n",
        "            FROM noticias\n",
        "            GROUP BY categoria\n",
        "            ORDER BY cantidad DESC\n",
        "        ''')\n",
        "\n",
        "        print(\"\\nüìà RESUMEN POR CATEGOR√çAS:\")\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "        for row in cursor.fetchall():\n",
        "            categoria, cantidad, relevancia = row\n",
        "            print(f\"{categoria}: {cantidad} noticias (Relevancia promedio: {relevancia:.1f})\")\n",
        "\n",
        "    def cerrar_conexion(self):\n",
        "        \"\"\"Cierra la conexi√≥n a la base de datos\"\"\"\n",
        "        self.conn.close()\n",
        "\n",
        "# Ejemplo de uso\n",
        "if __name__ == \"__main__\":\n",
        "    # Crear instancia del buscador\n",
        "    # Para usar NewsAPI, proporciona tu API key: BuscadorNoticias(news_api_key=\"tu_api_key\")\n",
        "    buscador = BuscadorNoticias()\n",
        "\n",
        "    try:\n",
        "        # Buscar noticias de los √∫ltimos 7 d√≠as\n",
        "        noticias = buscador.buscar_todas_las_fuentes(dias_atras=7)\n",
        "\n",
        "        # Guardar en base de datos\n",
        "        buscador.guardar_noticias(noticias)\n",
        "\n",
        "        # Mostrar resumen\n",
        "        buscador.mostrar_resumen()\n",
        "\n",
        "        # Generar reporte\n",
        "        buscador.generar_reporte('excel')\n",
        "\n",
        "        # Mostrar las 5 noticias m√°s relevantes\n",
        "        print(\"\\nüî• TOP 5 NOTICIAS M√ÅS RELEVANTES:\")\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "        noticias_ordenadas = sorted(noticias, key=lambda x: x['relevancia'], reverse=True)\n",
        "        for i, noticia in enumerate(noticias_ordenadas[:5], 1):\n",
        "            print(f\"{i}. [{noticia['categoria']}] {noticia['titulo']}\")\n",
        "            print(f\"   Fuente: {noticia['fuente']} | Relevancia: {noticia['relevancia']}/10\")\n",
        "            print(f\"   URL: {noticia['url']}\\n\")\n",
        "\n",
        "    finally:\n",
        "        buscador.cerrar_conexion()"
      ]
    }
  ]
}